import numpy as np
import torch
import yaml
from box import Box
import os
import lightning as L
import trimesh

# --- æ–°å¢çš„å¯¼å…¥ ---
from src.tokenizer.spec import DetokenizeOutput 
# --- ç»“æŸæ–°å¢ ---

from src.data.raw_data import RawData
from src.inference.download import download
from src.data.datapath import Datapath
from src.data.dataset import UniRigDatasetModule, DatasetConfig
from src.data.transform import TransformConfig
from src.tokenizer.parse import get_tokenizer
from src.tokenizer.spec import TokenizerConfig
from src.model.parse import get_model
from src.system.parse import get_system

# å…¨å±€å˜é‡ï¼Œç”¨äºè¢« hook å‡½æ•°ä¿®æ”¹
hidden_state_from_hook = None

def get_mlp_input_hook(module, input, output):
    """PyTorch hook å‡½æ•°ï¼Œç”¨äºæ•è·æŒ‡å®šç½‘ç»œå±‚çš„è¾“å…¥ã€‚"""
    global hidden_state_from_hook
    # print(f"âœ… Hook å·²è¢«è§¦å‘ï¼Œæ•è·åˆ°ç›®æ ‡å±‚çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º: {input[0].shape}")
    hidden_state_from_hook = input[0].detach().cpu()

def load(name: str, path: str) -> Box:
    """å®Œå…¨æ¨¡ä»¿ run.py ä¸­çš„ load å‡½æ•°ã€‚"""
    if path.endswith('.yaml'):
        path = path.removesuffix('.yaml')
    path += '.yaml'
    # print(f"\033[92måŠ è½½ {name} é…ç½®: {path}\033[0m")
    return Box(yaml.safe_load(open(path, 'r')))

def preprocess_obj_to_npz(obj_path: str, output_dir: str) -> str:
    """æ‰‹åŠ¨å°† .obj æ–‡ä»¶é¢„å¤„ç†ä¸º raw_data.npz æ–‡ä»¶ã€‚"""
    print(f"\nâš™ï¸  å¼€å§‹æ‰‹åŠ¨é¢„å¤„ç†: {obj_path}")
    try:
        mesh = trimesh.load_mesh(obj_path)
    except Exception as e:
        print(f"âŒ ä½¿ç”¨ trimesh åŠ è½½ .obj æ–‡ä»¶å¤±è´¥: {e}")
        return None

    raw_data_instance = RawData(
        vertices=np.array(mesh.vertices, dtype=np.float32),
        faces=np.array(mesh.faces, dtype=np.int64),
        vertex_normals=np.array(mesh.vertex_normals, dtype=np.float32),
        face_normals=np.array(mesh.face_normals, dtype=np.float32),
        joints=None, tails=None, skin=None, no_skin=None,
        parents=None, names=None, matrix_local=None,
    )
    
    os.makedirs(output_dir, exist_ok=True)
    npz_path = os.path.join(output_dir, 'raw_data.npz')
    raw_data_instance.save(npz_path)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆé¢„å¤„ç†æ–‡ä»¶: {npz_path}")
    return npz_path

# =====================================================================
# --- æœ€ç»ˆå®ç°çš„ BVH ä¿å­˜å‡½æ•° ---
# =====================================================================
def save_skeleton_to_bvh(skeleton_output: DetokenizeOutput, bvh_path: str):
    """
    å°† UniRig çš„éª¨æ¶è¾“å‡ºå¯¹è±¡ (DetokenizeOutput) ä¿å­˜ä¸º .bvh æ–‡ä»¶ã€‚

    Args:
        skeleton_output: ä» UniRig æ¨ç†å¾—åˆ°çš„ DetokenizeOutput å¯¹è±¡ã€‚
        bvh_path (str): è¾“å‡ºçš„ .bvh æ–‡ä»¶è·¯å¾„ã€‚
    """
    print(f"\nğŸ’¾  å¼€å§‹å°†éª¨éª¼ä¿å­˜ä¸º .bvh æ–‡ä»¶: {bvh_path}...")
    
    # 1. ä» DetokenizeOutput å¯¹è±¡ä¸­å®‰å…¨åœ°æå–æ•°æ®
    try:
        # ä½¿ç”¨ .joints å±æ€§è·å–å…³èŠ‚ä½ç½®
        joints_pos = skeleton_output.joints
        if isinstance(joints_pos, torch.Tensor):
            joints_pos = joints_pos.cpu().numpy()

        # ä½¿ç”¨ ._get_parents() æ–¹æ³•è·å–æœ€å¯é çš„çˆ¶å­å…³ç³»
        parents_idx = skeleton_output._get_parents()
        
        # ä½¿ç”¨ .names å±æ€§ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤åå­—
        joint_names = skeleton_output.names
        if joint_names is None or len(joint_names) != len(joints_pos):
            print("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°å…³èŠ‚åç§°æˆ–æ•°é‡ä¸åŒ¹é…ï¼Œå°†ä½¿ç”¨é»˜è®¤åç§° 'joint_N'ã€‚")
            joint_names = [f"joint_{i}" for i in range(len(joints_pos))]
            
        num_joints = joints_pos.shape[0]

    except Exception as e:
        print(f"âŒ é”™è¯¯ï¼šä»éª¨æ¶å¯¹è±¡ä¸­æå–æ•°æ®å¤±è´¥ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return

    # 2. æ„å»ºå†™å…¥ .bvh æ‰€éœ€çš„çˆ¶å­å…³ç³»æ ‘
    children = [[] for _ in range(num_joints)]
    root_idx = -1
    for i, p_idx in enumerate(parents_idx):
        if p_idx is None: # æ ¹èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹æ˜¯ None
            root_idx = i
        else:
            children[p_idx].append(i)

    if root_idx == -1 and len(joints_pos) > 0:
        print("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°æ ¹èŠ‚ç‚¹ï¼Œå°†å‡å®šç¬¬ä¸€ä¸ªå…³èŠ‚ä¸ºæ ¹èŠ‚ç‚¹ã€‚")
        root_idx = 0
    elif len(joints_pos) == 0:
        print("âŒ é”™è¯¯: éª¨æ¶ä¸­æ²¡æœ‰ä»»ä½•å…³èŠ‚ã€‚")
        return

    # 3. å¼€å§‹å†™å…¥æ–‡ä»¶
    with open(bvh_path, 'w') as f:
        f.write("HIERARCHY\n")

        def write_joint_hierarchy(joint_idx, indent_level):
            indent = "  " * indent_level
            is_root = (parents_idx[joint_idx] is None)
            
            f.write(f"{indent}ROOT {joint_names[joint_idx]}\n" if is_root else f"{indent}JOINT {joint_names[joint_idx]}\n")
            f.write(f"{indent}{{\n")

            if is_root:
                offset = joints_pos[joint_idx]
            else:
                parent_pos = joints_pos[parents_idx[joint_idx]]
                offset = joints_pos[joint_idx] - parent_pos
            
            f.write(f"{indent}  OFFSET {offset[0]:.6f} {offset[1]:.6f} {offset[2]:.6f}\n")
            f.write(f"{indent}  CHANNELS 3 Zrotation Yrotation Xrotation\n")

            if not children[joint_idx]: # æœ«ç«¯å…³èŠ‚
                f.write(f"{indent}  End Site\n")
                f.write(f"{indent}  {{\n")
                f.write(f"{indent}    OFFSET 0.0 0.0 0.0\n")
                f.write(f"{indent}  }}\n")
            else:
                for child_idx in children[joint_idx]:
                    write_joint_hierarchy(child_idx, indent_level + 1)
            
            f.write(f"{indent}}}\n")

        write_joint_hierarchy(root_idx, 0)

        f.write("\nMOTION\n")
        f.write(f"Frames: 1\n")
        f.write("Frame Time: 0.033333\n")
        motion_data = " ".join(["0.0"] * num_joints * 3)
        f.write(motion_data + "\n")

    print(f"âœ… æˆåŠŸä¿å­˜ .bvh æ–‡ä»¶: {bvh_path}")


def run_unirig_inference_final(processed_data_dir: str):
    """æœ€ç»ˆä¿®æ­£ç‰ˆï¼šè¯»å–é¢„å¤„ç†å¥½çš„ .npz æ–‡ä»¶å¹¶è¿›è¡Œæ¨ç†ã€‚"""
    # æ­¤å‡½æ•°å†…å®¹æ— éœ€ä¿®æ”¹ï¼Œä¿æŒåŸæ ·å³å¯
    global hidden_state_from_hook
    hidden_state_from_hook = None
    task_config_path = 'configs/task/quick_inference_skeleton_articulationxl_ar_256.yaml'
    task = load('task', task_config_path)
    data_config = load('data', os.path.join('configs/data', task.components.data))
    transform_config = load('transform', os.path.join('configs/transform', task.components.transform))
    tokenizer_config_path = task.components.get('tokenizer', None)
    if tokenizer_config_path:
        tokenizer_config = load('tokenizer', os.path.join('configs/tokenizer', task.components.tokenizer))
        tokenizer_config = TokenizerConfig.parse(config=tokenizer_config)
    else:
        tokenizer_config = None
    predict_dataset_config = DatasetConfig.parse(config=data_config.predict_dataset_config).split_by_cls()
    predict_transform_config = TransformConfig.parse(config=transform_config.predict_transform_config)
    datapath = Datapath(files=[processed_data_dir], cls=None)
    data_module = UniRigDatasetModule(
        process_fn=None, predict_dataset_config=predict_dataset_config,
        predict_transform_config=predict_transform_config, tokenizer_config=tokenizer_config,
        datapath=datapath,
    )
    model_config = load('model', os.path.join('configs/model', task.components.model))
    tokenizer = get_tokenizer(config=tokenizer_config) if tokenizer_config else None
    model = get_model(tokenizer=tokenizer, **model_config)
    data_module.process_fn = model._process_fn
    system_config = load('system', os.path.join('configs/system', task.components.system))
    system = get_system(**system_config, model=model, steps_per_epoch=1)
    
    try:
        target_layer = model.transformer.lm_head
        handle = target_layer.register_forward_hook(get_mlp_input_hook)
    except AttributeError:
        # æ­¤å¤„é”™è¯¯å¤„ç†å·²ä¸å†éœ€è¦ï¼Œä½†ä¿ç•™ä»¥é˜²ä¸‡ä¸€
        return None, None
    
    ckpt_path = download(task.get('resume_from_checkpoint', None))
    trainer_config = task.get('trainer', {})
    trainer = L.Trainer(**trainer_config)
    
    print(f"\nğŸš€ å¼€å§‹å¯¹ {processed_data_dir} è¿›è¡Œæ¨ç†...")
    predictions = trainer.predict(system, datamodule=data_module, ckpt_path=ckpt_path)
    
    handle.remove()
    return predictions, hidden_state_from_hook

# =====================================================================
# --- ä¸»æ‰§è¡Œå‡½æ•° ---
# =====================================================================
if __name__ == '__main__':
    # ç¡®ä¿è„šæœ¬åœ¨ UniRig é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    input_obj_file = '../BridgeForMotion/scripts/bear3EP_Agression_frame0.obj'
    
    # ä»è¾“å…¥è·¯å¾„ä¸­è·å–åŸºæœ¬ä¿¡æ¯
    file_basename = os.path.splitext(os.path.basename(input_obj_file))[0]
    input_dir = os.path.dirname(input_obj_file)
    
    # å®šä¹‰ä¸­é—´æ–‡ä»¶çš„è¾“å‡ºç›®å½•
    temp_npz_dir = os.path.join('temp_preprocess', file_basename)
    
    # å®šä¹‰æœ€ç»ˆçš„ BVH è¾“å‡ºè·¯å¾„ï¼Œä½¿å…¶ä¸è¾“å…¥æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹
    output_bvh_path = os.path.join(input_dir, f"{file_basename}_rig.bvh")

    if not os.path.exists(input_obj_file):
        print(f"âŒ [é”™è¯¯] è¾“å…¥çš„ .obj æ–‡ä»¶ä¸å­˜åœ¨: {input_obj_file}")
    else:
        # æ­¥éª¤ 1: é¢„å¤„ç†
        final_npz_path = preprocess_obj_to_npz(obj_path=input_obj_file, output_dir=temp_npz_dir)
        
        if final_npz_path:
            # æ­¥éª¤ 2: UniRig æ¨ç†
            predictions, hidden_state_X = run_unirig_inference_final(processed_data_dir=temp_npz_dir)
    
            # æ­¥éª¤ 3: æ£€æŸ¥ç»“æœå¹¶ä¿å­˜ BVH
            if hidden_state_X is not None and predictions and len(predictions) > 0 and len(predictions[0]) > 0:
                print("\n" + "="*30)
                print("ğŸ‰ æ­å–œï¼æ•´ä¸ªæµç¨‹å·²æ‰“é€šï¼ ğŸ‰")
                print("="*30)
                print(f"âœ… æ•è·åˆ°çš„éšè—çŠ¶æ€ (X) çš„å½¢çŠ¶: {hidden_state_X.shape}")
                
                # ä»åµŒå¥—åˆ—è¡¨ä¸­æå–å‡ºçœŸæ­£çš„ DetokenizeOutput å¯¹è±¡
                skeleton_obj = predictions[0][0]
                print(f"âœ… æˆåŠŸæå–éª¨æ¶å¯¹è±¡ï¼Œç±»å‹ä¸º: {type(skeleton_obj)}")
                
                # è°ƒç”¨æˆ‘ä»¬çš„æ–°å‡½æ•°æ¥ä¿å­˜ BVH æ–‡ä»¶
                save_skeleton_to_bvh(skeleton_obj, output_bvh_path)
                
            else:
                print("âŒ é”™è¯¯: æ¨ç†å®Œæˆä½†æœªèƒ½è·å–æœ‰æ•ˆç»“æœã€‚")
import numpy as np
import torch
import yaml
from box import Box
import os
import lightning as L
import trimesh
from src.data.raw_data import RawData

# ä» UniRig çš„ src ç›®å½•å¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°å’Œç±»
from src.inference.download import download
from src.data.datapath import Datapath
from src.data.dataset import UniRigDatasetModule, DatasetConfig
from src.data.transform import TransformConfig
from src.tokenizer.parse import get_tokenizer
from src.tokenizer.spec import TokenizerConfig
from src.model.parse import get_model
from src.system.parse import get_system

# --- ç¬¬2æ­¥ Déƒ¨åˆ†ï¼šæœ€ç»ˆã€æœ€å‡†ç¡®çš„æ¨ç†è„šæœ¬ ---

# å…¨å±€å˜é‡ï¼Œç”¨äºè¢« hook å‡½æ•°ä¿®æ”¹
hidden_state_from_hook = None

def get_mlp_input_hook(module, input, output):
    """PyTorch hook å‡½æ•°ï¼Œç”¨äºæ•è·æŒ‡å®šç½‘ç»œå±‚çš„è¾“å…¥ã€‚"""
    global hidden_state_from_hook
    print(f"âœ… Hook å·²è¢«è§¦å‘ï¼Œæ•è·åˆ°ç›®æ ‡å±‚çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º: {input[0].shape}")
    hidden_state_from_hook = input[0].detach().cpu()

def load(name: str, path: str) -> Box:
    """
    å®Œå…¨æ¨¡ä»¿ run.py ä¸­çš„ load å‡½æ•°ã€‚
    """
    if path.endswith('.yaml'):
        path = path.removesuffix('.yaml')
    path += '.yaml'
    print(f"\033[92måŠ è½½ {name} é…ç½®: {path}\033[0m")
    return Box(yaml.safe_load(open(path, 'r')))

def preprocess_obj_to_npz(obj_path: str, output_dir: str) -> str:
    """
    æ‰‹åŠ¨å°† .obj æ–‡ä»¶é¢„å¤„ç†ä¸º raw_data.npz æ–‡ä»¶ã€‚
    è¿™æ›¿ä»£äº† extract.py ä¸­å¯¹ Blender çš„ä¾èµ–ã€‚

    Args:
        obj_path (str): è¾“å…¥çš„ .obj æ–‡ä»¶è·¯å¾„ã€‚
        output_dir (str): ç”¨äºå­˜æ”¾ .npz æ–‡ä»¶çš„ç›®å½•ã€‚

    Returns:
        str: ç”Ÿæˆçš„ .npz æ–‡ä»¶çš„å®Œæ•´è·¯å¾„ã€‚
    """
    print(f"\nâš™ï¸  å¼€å§‹æ‰‹åŠ¨é¢„å¤„ç†: {obj_path}")
    
    # 1. ä½¿ç”¨ trimesh åŠ è½½ .obj æ–‡ä»¶
    try:
        mesh = trimesh.load_mesh(obj_path)
    except Exception as e:
        print(f"âŒ ä½¿ç”¨ trimesh åŠ è½½ .obj æ–‡ä»¶å¤±è´¥: {e}")
        return None

    # 2. å‡†å¤‡ RawData å¯¹è±¡æ‰€éœ€çš„æ•°æ®
    # ä»æˆ‘ä»¬æœ€åˆçš„ .anime æ–‡ä»¶è½¬æ¢è¿‡æ¥çš„ .obj åªåŒ…å«é¡¶ç‚¹å’Œé¢ç‰‡
    raw_data_instance = RawData(
        vertices=np.array(mesh.vertices, dtype=np.float32),
        faces=np.array(mesh.faces, dtype=np.int64),
        vertex_normals=np.array(mesh.vertex_normals, dtype=np.float32),
        face_normals=np.array(mesh.face_normals, dtype=np.float32),
        joints=None,
        tails=None,
        skin=None,
        no_skin=None,
        parents=None,
        names=None,
        matrix_local=None,
    )
    
    # 3. å®šä¹‰è¾“å‡ºè·¯å¾„å¹¶ä¿å­˜
    os.makedirs(output_dir, exist_ok=True)
    npz_path = os.path.join(output_dir, 'raw_data.npz')
    raw_data_instance.save(npz_path)
    
    print(f"âœ… æˆåŠŸç”Ÿæˆé¢„å¤„ç†æ–‡ä»¶: {npz_path}")
    return npz_path

def run_unirig_inference_final(processed_data_dir: str):
    """
    æœ€ç»ˆä¿®æ­£ç‰ˆï¼šä¸¥æ ¼æŒ‰ç…§ run.py çš„é€»è¾‘ï¼Œå¯¹å•ä¸ª obj æ–‡ä»¶è¿›è¡Œæ¨ç†ã€‚
    """
    global hidden_state_from_hook
    hidden_state_from_hook = None # æ¯æ¬¡è°ƒç”¨å‰é‡ç½®

    # 1. è®¾ç½®å‚æ•° (æ›¿ä»£ argparse)
    # ä½¿ç”¨æ‚¨æ‰¾åˆ°çš„æ­£ç¡®çš„ task é…ç½®æ–‡ä»¶ï¼
    task_config_path = 'configs/task/quick_inference_skeleton_articulationxl_ar_256.yaml'
    
    # 2. åŠ è½½é…ç½®å¹¶æ„å»ºç»„ä»¶ (ä¸¥æ ¼æ¨¡ä»¿ run.py)
    task = load('task', task_config_path)
    
    # --- é€ä¸€åŠ è½½æ‰€æœ‰ç»„ä»¶é…ç½® ---
    data_config = load('data', os.path.join('configs/data', task.components.data))
    transform_config = load('transform', os.path.join('configs/transform', task.components.transform))
    tokenizer_config_path = task.components.get('tokenizer', None)
    if tokenizer_config_path:
        tokenizer_config = load('tokenizer', os.path.join('configs/tokenizer', task.components.tokenizer))
        tokenizer_config = TokenizerConfig.parse(config=tokenizer_config)
    else:
        tokenizer_config = None
        
    predict_dataset_config = DatasetConfig.parse(config=data_config.predict_dataset_config).split_by_cls()
    predict_transform_config = TransformConfig.parse(config=transform_config.predict_transform_config)

    # --- æ„å»º DataModule ---
    datapath = Datapath(files=[processed_data_dir], cls=None)
    data_module = UniRigDatasetModule(
        process_fn=None,
        predict_dataset_config=predict_dataset_config,
        predict_transform_config=predict_transform_config,
        tokenizer_config=tokenizer_config,
        datapath=datapath,
    )
    
    # --- æ„å»º Model ---
    model_config = load('model', os.path.join('configs/model', task.components.model))
    tokenizer = get_tokenizer(config=tokenizer_config) if tokenizer_config else None
    model = get_model(tokenizer=tokenizer, **model_config)
    data_module.process_fn = model._process_fn # å…³è” model çš„å¤„ç†å‡½æ•°åˆ° datamodule
    
    # --- æ„å»º System ---
    system_config = load('system', os.path.join('configs/system', task.components.system))
    system = get_system(**system_config, model=model, steps_per_epoch=1)

    # 3. ã€å…³é”®ä»»åŠ¡ã€‘æ‰¾åˆ°ç›®æ ‡å±‚å¹¶æ³¨å†Œ Hook
    # è¿™ä¸ªé€»è¾‘ä¸å˜ï¼Œä½†ç°åœ¨æˆ‘ä»¬æ“ä½œçš„ model å¯¹è±¡æ˜¯é€šè¿‡å®Œå…¨æ­£ç¡®çš„æ–¹å¼æ„å»ºçš„
    try:
        # â—â—â— è¿™æ˜¯æ‚¨éœ€è¦åšçš„æœ€åä¸€æ­¥ä¾¦æŸ¥å·¥ä½œ â—â—â—
        # å–æ¶ˆä¸‹é¢ä¸€è¡Œçš„æ³¨é‡Šæ¥æ‰“å°æ¨¡å‹ç»“æ„ï¼Œç„¶åæ‰¾åˆ°æ­£ç¡®çš„å±‚
        # print(model) 
        
        target_layer = model.transformer.lm_head # <--- ï¼ï¼ï¼è¯·æ ¹æ®æ‰“å°å‡ºçš„ç»“æ„ï¼Œæ›¿æ¢æˆçœŸå®çš„è·¯å¾„ï¼ï¼ï¼
        print(f"âœ… æˆåŠŸæ‰¾åˆ°ç›®æ ‡å±‚: {target_layer}")
        handle = target_layer.register_forward_hook(get_mlp_input_hook)
    except AttributeError as e:
        print(f"âŒ [é”™è¯¯] æ— æ³•æ‰¾åˆ°æŒ‡å®šçš„ç›®æ ‡å±‚ã€‚è¯·å–æ¶ˆä¸Šé¢ `print(model)` çš„æ³¨é‡Šï¼Œ")
        print("   è¿è¡Œè„šæœ¬ï¼Œæ£€æŸ¥æ‰“å°å‡ºçš„æ¨¡å‹ç»“æ„ï¼Œç„¶åæ›´æ­£ `target_layer` çš„è·¯å¾„ã€‚")
        return None, None

    # 4. æ‰§è¡Œæ¨ç† (ä½¿ç”¨ PyTorch Lightning Trainer)
    # è·å–é¢„è®­ç»ƒæ¨¡å‹æƒé‡è·¯å¾„å¹¶è‡ªåŠ¨ä¸‹è½½
    ckpt_path = task.get('resume_from_checkpoint', None)
    if ckpt_path:
        ckpt_path = download(ckpt_path)
    else:
        print("âŒ [é”™è¯¯] åœ¨ task é…ç½®æ–‡ä»¶ä¸­æœªæ‰¾åˆ° `resume_from_checkpoint` è·¯å¾„ï¼")
        return None, None
    
    trainer_config = task.get('trainer', {})
    trainer = L.Trainer(**trainer_config)

    print(f"\nğŸš€ å¼€å§‹å¯¹ {processed_data_dir} è¿›è¡Œæ¨ç†...")
    predictions = trainer.predict(system, datamodule=data_module, ckpt_path=ckpt_path)
    
    handle.remove() # å¸è½½ hook

    # 5. æå–å¹¶è¿”å›ç»“æœ
    # æ³¨æ„ï¼š`trainer.predict()` çš„è¿”å›å€¼å¯èƒ½ä¸ºç©ºåˆ—è¡¨ `[]`ï¼Œå› ä¸ºç»“æœé€šå¸¸ç”±ä¸€ä¸ª "writer" å›è°ƒå‡½æ•°ç›´æ¥å†™å…¥æ–‡ä»¶ã€‚
    # ä½†è¿™æ²¡å…³ç³»ï¼Œæˆ‘ä»¬çš„ä¸»è¦ç›®æ ‡ `hidden_state_from_hook` å·²ç»è¢« hook å‡½æ•°æ•è·äº†ã€‚
    predicted_skeleton_obj = predictions
    
    if hidden_state_from_hook is None:
        print("âš ï¸ è­¦å‘Š: æ¨ç†å®Œæˆï¼Œä½† hook æœªæ•è·åˆ°ä»»ä½•æ•°æ®ã€‚è¯·æ£€æŸ¥ target_layer è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        
    return predicted_skeleton_obj, hidden_state_from_hook

if __name__ == '__main__':
    # ç¡®ä¿è„šæœ¬åœ¨ UniRig é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    input_obj_file = '../BridgeForMotion/scripts/bear3EP_Agression_frame0.obj'
    
    # ä¸ºæˆ‘ä»¬çš„ npz æ–‡ä»¶åˆ›å»ºä¸€ä¸ªä¸´æ—¶è¾“å‡ºç›®å½•
    # è·¯å¾„ç»“æ„æ¨¡ä»¿ UniRig çš„é¢„æœŸï¼š<output_dir>/<file_basename>/raw_data.npz
    file_basename = os.path.splitext(os.path.basename(input_obj_file))[0]
    temp_npz_dir = os.path.join('temp_preprocess', file_basename)

    if not os.path.exists(input_obj_file):
        print(f"âŒ [é”™è¯¯] è¾“å…¥çš„ .obj æ–‡ä»¶ä¸å­˜åœ¨: {input_obj_file}")
    else:
        # 1. åœ¨æ¨ç†å‰ï¼Œå…ˆæ‰§è¡Œé¢„å¤„ç†ï¼Œç”Ÿæˆ .npz æ–‡ä»¶
        final_npz_path = preprocess_obj_to_npz(obj_path=input_obj_file, output_dir=temp_npz_dir)
        
        if final_npz_path:
            # 2. å°†ç”Ÿæˆçš„ .npz æ–‡ä»¶æ‰€åœ¨çš„ã€ç›®å½•ã€‘ä¼ é€’ç»™ UniRig çš„æ¨ç†å‡½æ•°
            # è¿™æ˜¯å…³é”®ï¼UniRig çš„æ•°æ®åŠ è½½å™¨éœ€è¦çš„æ˜¯åŒ…å« npz çš„ç›®å½•è·¯å¾„
            predicted_skeleton, hidden_state_X = run_unirig_inference_final(processed_data_dir=temp_npz_dir)
    
            if hidden_state_X is not None:
                print("\n" + "="*30)
                print("ğŸ‰ æ­å–œï¼æ•´ä¸ªæµç¨‹å·²æ‰“é€šï¼ ğŸ‰")
                print("="*30)
                print(f"âœ… æ•è·åˆ°çš„éšè—çŠ¶æ€ (X) çš„å½¢çŠ¶: {hidden_state_X.shape}")
                print(f"âœ… æ¨ç†å‡½æ•°è¿”å›çš„éª¨æ¶å¯¹è±¡: {predicted_skeleton}")